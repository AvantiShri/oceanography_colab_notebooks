{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ESS Project 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvantiShri/oceanography_colab_notebooks/blob/master/classes/estimating_npp_from_satellite_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-0I53jkVpY_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "5ac9037e-fa37-4e3d-c111-2cf5866365a5"
      },
      "source": [
        "#Install necessary python packages\n",
        "!pip install netCDF4\n",
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting netCDF4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/1b/2fcb6c6b34cf4b16ba59acefad5329df7979600b7c6ab24acbe43dbf1f27/netCDF4-1.5.0.1-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 4.0MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from netCDF4) (1.16.2)\n",
            "Collecting cftime (from netCDF4)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/64/8ceadda42af3c1b27ee77005807e38c6d77baef28a8f9216b60577fddd71/cftime-1.0.3.4-cp36-cp36m-manylinux1_x86_64.whl (305kB)\n",
            "\u001b[K    100% |████████████████████████████████| 307kB 27.1MB/s \n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.0.3.4 netCDF4-1.5.0.1\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGfOtNyTp1XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "import netCDF4\n",
        "import os\n",
        "from netCDF4 import Dataset\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "def radians(deg):\n",
        "  return (deg/180.0)*(np.pi)\n",
        "\n",
        "def degrees(rad):\n",
        "  return 180.0*(rad/(np.pi))\n",
        "\n",
        "def area_of_patch(lat1,lat2,lon1,lon2):\n",
        "  earth_radius = 6378100 #meters\n",
        "  return (np.abs(np.sin(radians(lat2)) - np.sin(radians(lat1)))\n",
        "          *2*np.pi*(earth_radius**2)*(np.abs(lon2-lon1)/360.0))\n",
        "\n",
        "def daylength_calc(julian_day, lat):\n",
        "  date_angle_rad = radians(360.0*(julian_day/365.0))\n",
        "  \n",
        "  decl_deg = (0.39637\n",
        "              -22.9133*np.cos(date_angle_rad)\n",
        "              +4.02543*np.sin(date_angle_rad)\n",
        "              -0.3872*np.cos(2*date_angle_rad)\n",
        "              +0.052*np.sin(2*date_angle_rad))\n",
        "  decl_rad = -radians(decl_deg)\n",
        "  lat_rad = -radians(lat)\n",
        "  \n",
        "  tan_prod = -np.tan(lat_rad)*np.tan(decl_rad)\n",
        "  if (np.isnan(np.arccos(tan_prod))==False):\n",
        "    return 0.133*degrees(np.arccos(tan_prod))\n",
        "  else:\n",
        "    if (np.abs(tan_prod)==tan_prod):\n",
        "      return 0\n",
        "    else:\n",
        "      return 24\n",
        "\n",
        "def format_day_string(day):\n",
        "  return \"\".join([\"0\" for i in range(3-len(str(day)))]) + str(day)\n",
        "\n",
        "def read_map_dataset(file, data_key):\n",
        "  dataset = Dataset(file)\n",
        "\n",
        "  data = np.array(dataset[data_key])\n",
        "  mask = (data > dataset[data_key]._FillValue)\n",
        "  data[mask==False] = np.nan\n",
        "  latitudes = np.array(dataset['lat'])\n",
        "  longitudes = np.array(dataset['lon'])\n",
        "  assert len(latitudes) == data.shape[0]\n",
        "  assert len(longitudes) == data.shape[1]\n",
        "  return data, mask, latitudes, longitudes\n",
        "  \n",
        "def npp_integral(Pmax, K, z, Eo, Ec):\n",
        "  \n",
        "  #Half Saturation Constant; Ek = 30 µEin m-2 s-1\n",
        "  Ek = 30 #µEin m-2 s-1\n",
        "  \n",
        "  #5.\tThen calculate NPP (Pn) using the following equation that accounts\n",
        "  # for phytoplankton photophysiology and changes in the light\n",
        "  # environment with depth (Ez):\n",
        "  # Pn = [Pmax * (Ez - Ec)] / [Ek + (Ez - Ec)]\n",
        "  \n",
        "  ##We will take the integral.\n",
        "  ##Rearranging:\n",
        "  # Pn = [Pmax]/[Ek/(Ez - Ec) + 1]\n",
        "  ##Substituting Beer's law: Ez = E0 * exp(-Kz)\n",
        "  # Pn = [Pmax]/[Ek/(E0*exp(-Kz) - Ec) + 1]\n",
        "  ##After plugging into Wolfram Alpha:\n",
        "  # (Pmax)/(K(Ec-Ek)) * [Ek*log(Ek - Ec + E0*exp(-Kz)) + Ec*K*z]\n",
        "  \n",
        "  ##Confirm the integral is right by taking the derivative:\n",
        "  # d/dz (Pmax/(K(Ec-Ek)))*[Ek*log(Ek - Ec + E0*exp(-Kz)) + Ec*K*z]\n",
        "  ##Apply the chain rule:\n",
        "  # = (Pmax/(K(Ec-Ek)))*[-Ek*E0*K*exp(-Kz)/[Ek - Ec + E0*exp(-Kz)] + Ec*K]\n",
        "  ##Cancel out K\n",
        "  # = (Pmax/(Ec-Ek))*[-Ek*E0*exp(-Kz)/[Ek - Ec + E0*exp(-Kz)] + Ec]\n",
        "  ##Replace E0*exp(-Kz) with Ez \n",
        "  # = (Pmax/(Ec-Ek))*[-Ek*Ez/[Ek - Ec + Ez] + Ec]\n",
        "  ##Unify the denominator into a single term\n",
        "  # = (Pmax/(Ec-Ek))*[(-Ek*Ez + Ec*(Ek - Ec + Ez)) /[Ek - Ec + Ez]]\n",
        "  ##Refactor the denominator\n",
        "  # = (Pmax/(Ec-Ek))*[(-Ek*Ez + Ec*Ez + Ec*(Ek - Ec)) /[Ek - Ec + Ez]]\n",
        "  # = (Pmax/(Ec-Ek))*[((Ec-Ek)*Ez + Ec*(Ek - Ec)) /[Ek - Ec + Ez]]\n",
        "  # = (Pmax/(Ec-Ek))*[((Ec-Ek)*Ez - Ec*(Ec - Ek)) /[Ek - Ec + Ez]]\n",
        "  ##Cancel out Ec-Ek\n",
        "  # = Pmax*[(Ez - Ec) /[Ek - Ec + Ez]]\n",
        "  ## It's the same!\n",
        "  # = Pmax*(Ez - Ec)/[Ek - (Ez-Ec)]\n",
        "\n",
        "  return ((Pmax/(K*(Ec-Ek)))\n",
        "          *((Ek*np.log(Ek - Ec + Eo*np.exp(-K*z))) + Ec*K*z))\n",
        "\n",
        "\n",
        "def process_data_for_month(start_year, start_day, end_year, end_day,\n",
        "                           save_intermediate_data=False):\n",
        "  \n",
        "  print(\"On\",start_year,start_day,\"to\",end_year,end_day)\n",
        "  \n",
        "  date_string = (str(start_year)+format_day_string(start_day)\n",
        "                 +str(end_year)+format_day_string(end_day))\n",
        "  \n",
        "  chla_file = \"A\"+date_string+\".L3m_MO_CHL_chlor_a_4km.nc\"\n",
        "  par_file = \"A\"+date_string+\".L3m_MO_PAR_par_4km.nc\"\n",
        "  sst_file = \"A\"+date_string+\".L3m_MO_SST_sst_4km.nc\"\n",
        "  \n",
        "  #download the files\n",
        "  for file in [chla_file, par_file, sst_file]:\n",
        "    if (os.path.isfile(file)==False):\n",
        "      print(\"Downloading\",file)\n",
        "      wget.download(url=\"https://oceandata.sci.gsfc.nasa.gov/cgi/getfile/\"+file,\n",
        "                    out=file)\n",
        "      \n",
        "  chla_data, chla_mask, chla_lat, chla_lon = read_map_dataset(\n",
        "                                             file=chla_file, data_key='chlor_a')\n",
        "  par_data, par_mask, par_lat, par_lon = read_map_dataset(\n",
        "                                             file=par_file, data_key='par')\n",
        "  sst_data, sst_mask, sst_lat, sst_lon = read_map_dataset(\n",
        "                                             file=sst_file, data_key='sst')\n",
        "  #os.remove(chla_file)\n",
        "  #os.remove(par_file)\n",
        "  #os.remove(sst_file)\n",
        "  \n",
        "  lats = chla_lat\n",
        "  lons = chla_lon\n",
        "  #double-check that the latitudes and longitudes are consistent across all the\n",
        "  # datasets\n",
        "  assert np.sum(np.abs(lats-chla_lat))==0, np.sum(np.abs(lats-chla_lat))\n",
        "  assert np.sum(np.abs(lats-par_lat))==0, np.sum(np.abs(lats-par_lat))\n",
        "  assert np.sum(np.abs(lats-sst_lat))==0, np.sum(np.abs(lats-sst_lat))\n",
        "  assert np.sum(np.abs(lons-chla_lon))==0, np.sum(np.abs(lons-chla_lon))\n",
        "  assert np.sum(np.abs(lons-par_lon))==0, np.sum(np.abs(lons-par_lon))\n",
        "  assert np.sum(np.abs(lons-sst_lon))==0, np.sum(np.abs(lons-sst_lon))\n",
        "\n",
        "  \n",
        "  #combined_mask is the mask that represents all unavailable values.\n",
        "  # \"True\" means the value is available, \"False\" means it's not.\n",
        "  combined_mask = chla_mask*par_mask*sst_mask\n",
        "  \n",
        "  \n",
        "  #Diffuse Attenuation Coefficient; K = 0.04 + 0.05 * [Chl a]0.681 (m-1)\n",
        "  K = 0.04 + 0.05*np.power(chla_data,0.681) #m-1\n",
        "  \n",
        "  \n",
        "  #Assume the mixed layer depth equals the euphotic depth.\n",
        "  # Think about what the euphotic depth is and how you will calculate it.\n",
        "  #Ans: euphotic depth is generally taken to be the depth at which irradiance\n",
        "  # is 1% of surface levels. Some exceptions are made in oligotrophic gyres,\n",
        "  # but these gyres don't have a lot of primary production so we will just\n",
        "  # use 1%. But we also need to make sure that we are above the compensation\n",
        "  # intensity\n",
        "  #Compensation Intensity; Ec = 10 µEin m-2 s-1\n",
        "  Ec = 10 #µEin m-2 s-1\n",
        "  euphotic_depth_light_frac = 0.01\n",
        "  \n",
        "  #1a-c:\tEstimate the mean surface irradiance, Eo (µEin m-2 s-1),\n",
        "  # for each of your stations and seasons.\n",
        "  #Answer: this is par_data; we obtained it directly\n",
        "  # but the par data is for the whole day, so we are going to use some\n",
        "  # formulas from the activity to estimate the noon irradiance\n",
        "  \n",
        "  Eo_daily_integral = par_data*1000000 #convert from Einstiens to µEin \n",
        "  \n",
        "  #Get the mean daylength from the period start_day to end_day\n",
        "  lat_mean_daylengths = np.mean(np.array([[\n",
        "    daylength_calc(julian_day=y, lat=x) for x in lats]\n",
        "    for y in range(start_day, end_day+1)]), axis=0)\n",
        "  \n",
        "  #divide by (24 - photoperiod)*3600 to get average irradiance per second.\n",
        "  Eo_avg_persec = Eo_daily_integral/(\n",
        "                   1e-7 + (24 - lat_mean_daylengths[:,None])*3600)\n",
        "  \n",
        "  #assuming irradiance varies as described in handout\n",
        "  # from noon to end of photoperiod, we can divide by 0.5 to\n",
        "  # get irradiance at noon\n",
        "  Eo = 2*Eo_avg_persec\n",
        "  \n",
        "  #2.\tYou will be calculating depth-integrated NPP based\n",
        "  # on surface conditions of Chl a, light, and temperature.\n",
        "  # Before you get started, discuss with your group\n",
        "  # how many depths you should use.  \n",
        "  #Answer: we need to solve for the depth that has 1% of surface irradiance\n",
        "  # or has intensity equal to the compensation intensity, whichever is smaller.\n",
        "  # Beer’s law:  Ez = E0 * exp(-Kz)\n",
        "  # exp(-Kz) = 0.01\n",
        "  # -Kz = log(0.01)\n",
        "  # z = -log(0.01)/K\n",
        "  euphotic_depth = np.minimum(-np.log(euphotic_depth_light_frac),\n",
        "                              -np.log(Ec/Eo))/K #in meters\n",
        "  \n",
        "  #3.\tCalculate light penetration through the water column using the\n",
        "  # Beer-Lambert law: Beer’s law:  Ez = E0 * exp(-Kz)\n",
        "  #Ans: we will just incorporate Beer's law later as part of the integral\n",
        "  \n",
        "  #4. Calculate maximum rate of photosynthesis, Pmax (mg C mg-1 Chl a h-1),\n",
        "  # from sea surface temperature (T) using: Pmax = 1.8 * exp(0.0633*T) \n",
        "  Pmax = 1.8*np.exp(0.0633*sst_data)*chla_data #Assuming T is in deg C\n",
        "  \n",
        "  #5.\tThen calculate NPP (Pn) using the following equation that accounts\n",
        "  # for phytoplankton photophysiology and changes in the light\n",
        "  # environment with depth (Ez):\n",
        "  # Pn = [Pmax * (Ez - Ec)] / [Ek + (Ez - Ec)]\n",
        "  \n",
        "  npp_noon = (npp_integral(Pmax=Pmax, K=K, z=euphotic_depth, Eo=Eo, Ec=Ec)\n",
        "              - npp_integral(Pmax=Pmax, K=K, z=0, Eo=Eo, Ec=Ec))\n",
        "  \n",
        "  #Step 3\n",
        "  # Use the spreadsheet \"daylength_calc.xls\" to calculate the hours of\n",
        "  # sunlight per day (daylength) at a specific latitude for a specific day.\n",
        "  # After you’ve calculated this, double check your daylength results\n",
        "  # to see if this makes sense with what you know about your region’s\n",
        "  # latitude and hemisphere\n",
        "  \n",
        "  \n",
        "  \n",
        "  #1. Integrate NPP over the photoperiod (F) to obtain daily NPP,\n",
        "  # taking into account the latitude and season. Since E0 is measured\n",
        "  # at noon, scale your NPP value using the relationship:\n",
        "  # PP (mg C m-2 d-1) = 0.5 F*(PPnoon – PPnoon+0.5F)\n",
        "  \n",
        "  # We assume PP_{noon+0.5F} = 0 due to no irradiance, so:\n",
        "  npp_per_day = 0.5*lat_mean_daylengths[:,None]*npp_noon\n",
        "  npp_whole_period = npp_per_day*(end_day - start_day)\n",
        "  \n",
        "  if (save_intermediate_data):\n",
        "    file_name = (\"data_\"\n",
        "                  +str(start_year)+\"_\"+str(start_day)\n",
        "                  +\"_to_\"+str(end_year)+\"_\"+str(end_day))+\".h5\"\n",
        "    if (os.path.isfile(file_name)):\n",
        "      os.remove(file_name)\n",
        "    f = h5py.File(file_name)\n",
        "    f.create_dataset(\"chla\", data=chla_data)\n",
        "    f.create_dataset(\"Eo\", data=Eo)\n",
        "    f.create_dataset(\"sst\", data=sst_data)\n",
        "    f.create_dataset(\"lats\", data=lats)\n",
        "    f.create_dataset(\"lons\", data=lons)\n",
        "    f.create_dataset(\"mask\", data=combined_mask),\n",
        "    f.create_dataset(\"K\", data=K)\n",
        "    f.create_dataset(\"euphotic_depth\", data=euphotic_depth)\n",
        "    f.create_dataset(\"Pmax\", data=Pmax)\n",
        "    f.create_dataset(\"npp_noon\", data=npp_noon)\n",
        "    f.create_dataset(\"lat_mean_daylengths\", data=lat_mean_daylengths)\n",
        "    f.create_dataset(\"npp_whole_period\", data=npp_whole_period)\n",
        "    f.close()\n",
        "  \n",
        "  return npp_whole_period, lats, lons\n",
        "\n",
        "  #print(np.nanmax(chla_data), np.nanmin(chla_data))\n",
        "  #from matplotlib import pyplot as plt\n",
        "  #import seaborn as sns\n",
        "  #sns.heatmap(np.log(1+chla_data), xticklabels=False, yticklabels=False)\n",
        "  #plt.imshow(chla_data[1500:2000, 0:1000], vmin=np.nanmin(chla_data), vmax=np.nanmax(chla_data))\n",
        "  #plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mssd78Xq7P4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "28b79bfd-bf0d-4eef-d138-bb9640d81ea6"
      },
      "source": [
        "#date ranges for the months in terms of julian days\n",
        "julian_date_ranges = [\n",
        "    (1, 31),\n",
        "    (32, 59),\n",
        "    (60, 90),\n",
        "    (91, 120),\n",
        "    (121, 151),\n",
        "    (152, 181),\n",
        "    (182, 212),\n",
        "    (213, 243),\n",
        "    (244, 273),\n",
        "    (274, 304),\n",
        "    (305, 334),\n",
        "    (335, 365)  \n",
        "]\n",
        "\n",
        "npps_list = []\n",
        "lats_list = []\n",
        "lons_list = []\n",
        "\n",
        "for (start_day, end_day) in julian_date_ranges:\n",
        "  npp_whole_period, lats, lons =\\\n",
        "    process_data_for_month(2003, start_day, 2003, end_day)\n",
        "  npps_list.append(npp_whole_period)\n",
        "  lats_list.append(lats)\n",
        "  lons_list.append(lons)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On 2003 1 to 2003 31\n",
            "Downloading A20030012003031.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20030012003031.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20030012003031.L3m_MO_SST_sst_4km.nc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in arccos\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:188: RuntimeWarning: divide by zero encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "On 2003 32 to 2003 59\n",
            "Downloading A20030322003059.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20030322003059.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20030322003059.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 60 to 2003 90\n",
            "Downloading A20030602003090.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20030602003090.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20030602003090.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 91 to 2003 120\n",
            "Downloading A20030912003120.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20030912003120.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20030912003120.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 121 to 2003 151\n",
            "Downloading A20031212003151.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20031212003151.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20031212003151.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 152 to 2003 181\n",
            "Downloading A20031522003181.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20031522003181.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20031522003181.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 182 to 2003 212\n",
            "Downloading A20031822003212.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20031822003212.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20031822003212.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 213 to 2003 243\n",
            "Downloading A20032132003243.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20032132003243.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20032132003243.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 244 to 2003 273\n",
            "Downloading A20032442003273.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20032442003273.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20032442003273.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 274 to 2003 304\n",
            "Downloading A20032742003304.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20032742003304.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20032742003304.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 305 to 2003 334\n",
            "Downloading A20033052003334.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20033052003334.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20033052003334.L3m_MO_SST_sst_4km.nc\n",
            "On 2003 335 to 2003 365\n",
            "Downloading A20033352003365.L3m_MO_CHL_chlor_a_4km.nc\n",
            "Downloading A20033352003365.L3m_MO_PAR_par_4km.nc\n",
            "Downloading A20033352003365.L3m_MO_SST_sst_4km.nc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0y62DNKMeV2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sum up the npp over all the months\n",
        "sum_npp = np.sum(npps_list, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iBUtHFIMoDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#verify that all the latitudes and longitudes are compatible\n",
        "for lats in lats_list:\n",
        "  assert np.sum(np.abs(lats-lats_list[0]))==0\n",
        "for lons in lons_list:\n",
        "  assert np.sum(np.abs(lons-lons_list[0]))==0\n",
        "\n",
        "lats = lats_list[0]\n",
        "lons = lons_list[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YNaWLkPU-pb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04f3d498-5316-4a5c-a4fa-454b2054be2a"
      },
      "source": [
        "#create an array that stores the area of a particular latitude-longitude\n",
        "# patch, accounting for the curvature of the earth\n",
        "area_grid = area_of_patch(lat1=lats[:,None],\n",
        "                          lat2=np.array(list(lats[1:])+[-90.0])[:,None],\n",
        "                          lon1=lons[None,:],\n",
        "                          lon2=np.array(list(lons[1:])+[180.0])[None,:] )\n",
        "print(\"Sanity check - total area of earth in m2:\",np.sum(area_grid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sanity check - total area of earth in m2: 511172633667834.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRkSVFPUNF9-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create a mask that selects coordinates that correspond to the north pacific\n",
        "north_pacific_mask = (((lats > 0)*(lats < 60))[:,None])*((np.abs(lons) > 105)[None,:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hwdrtHUYu26",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ed216b4-f6d4-4ec3-89b5-2f92a7735e2e"
      },
      "source": [
        "total_production = np.nansum(sum_npp*area_grid*north_pacific_mask)\n",
        "print(total_production/(1.0e15)) #in Tg C per year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3970.2361367876288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJgkngy_Y-kn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3621bf1-bdb8-42e7-9e78-adaea8138639"
      },
      "source": [
        "average_production_m2 = total_production/np.nansum(area_grid*north_pacific_mask)\n",
        "print(average_production_m2/1000.0) #in gC/m2 per year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43.04232671515516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9czWDCilP8Xo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92cb3c57-2b17-4c39-9a97-8b31b1d9922c"
      },
      "source": [
        "np.nansum(area_grid*north_pacific_mask)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92240276950216.83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI7i7nEd7_7H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dfecbabe-6a23-48a3-dc8b-0eec00740d56"
      },
      "source": [
        "world_production = np.nansum(sum_npp*area_grid)\n",
        "print(world_production/(1.0e15)) #in Tg C per year"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20795.250697877585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOfu3h6-98mF",
        "colab_type": "text"
      },
      "source": [
        "General overview of reading in NetCDF4 files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KWOnJgTZNjr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a280ed5-9c0d-4d61-f44a-a35cfa816403"
      },
      "source": [
        "\n",
        "from netCDF4 import Dataset\n",
        "\n",
        "dataset = Dataset(\"A20030012003031.L3m_MO_CHL_chlor_a_4km.nc\")\n",
        "print(\"types of entries:\",list(dataset.variables.keys()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "types of entries: ['chlor_a', 'lat', 'lon', 'palette']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVWL6jk25tW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c972a1ea-053d-4a91-e04b-44cb8871b22d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "chla_data = np.array(dataset['chlor_a'])\n",
        "lats = np.array(dataset['lat'])\n",
        "lons = np.array(dataset['lon'])\n",
        "print(\"chla data shape:\",chla_data.shape)\n",
        "print(\"lats shape:\",lats.shape)\n",
        "print(\"lons shape:\",lons.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chla data shape: (4320, 8640)\n",
            "lats shape: (4320,)\n",
            "lons shape: (8640,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCE1YxxC5upE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb646333-cc8b-44ab-895b-30a88354a62b"
      },
      "source": [
        "print(\"latitude:\", lats[1000], \"longitude:\", lons[500],\n",
        "      \"value:\", chla_data[1000, 500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "latitude: 48.3125 longitude: -159.14584 value: 0.29979503\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEpbMiDu6gai",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f15c5fc1-a453-449f-fbbe-7090e9ab5852"
      },
      "source": [
        "print(\"masked value:\", dataset['chlor_a']._FillValue)\n",
        "print(chla_data[0, 500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "masked value: -32767.0\n",
            "-32767.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHBtj9i_60Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}